{"cells":[{"cell_type":"markdown","source":["# End to End Pure Streaming Data-Pipeline for Apartment Maintenance Table Using Spark Structured Streaming on Databricks"],"metadata":{}},{"cell_type":"markdown","source":["###### Description: In this notebook we read apt_maintenance state rows from incoming csv files into a streamig dataframe, transform (clean, cast, rename) the data, add/update the latest state to a Databricks Delta table\n###### Objective: (incoming csv files) --> \"apt_maintenance_streamingDF\" --> \"results_df\" --> \"apt_maintenance_data\""],"metadata":{}},{"cell_type":"code","source":["import requests\nimport json\nimport optimus as op\nimport phonenumbers \nimport re\nimport datetime\nimport time\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext, Row\nfrom pyspark.sql.functions import unix_timestamp, from_unixtime\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window as W\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import rank, col"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Schema for Apartment Maintenance\napt_maintenance_schema = StructType([\n            StructField(\"Maintenance_id\", IntegerType(), False),\n            StructField(\"Apartment_number\", IntegerType(), True),\n            StructField(\"Mdate\", TimestampType(), False),\n            StructField(\"Issue_reported\", StringType(), True),\n            StructField(\"Contractor_id\", IntegerType(), True), \n            StructField(\"Resolution\", StringType(), True), \n            StructField(\"Status\", StringType(), True),\n            StructField(\"Charges_incurred\", StringType(), True),\n            StructField(\"event_time\", TimestampType(), True)])\n\napt_maintenance_udf_schema = StructType([\n            StructField(\"Apartment_number\", IntegerType(), True),\n            StructField(\"Mdate\", TimestampType(), False),\n            StructField(\"Issue_reported\", StringType(), True),\n            StructField(\"Contractor_id\", IntegerType(), True), \n            StructField(\"Resolution\", StringType(), True), \n            StructField(\"Status\", StringType(), True),\n            StructField(\"Charges_incurred\", StringType(), True),\n            StructField(\"event_time\", TimestampType(), True)])"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["###### Description: Get apt_maintenance csv files as a streaming \"apt_maintenance_streamingDF\" and process it on the fly and get transformed stream \"apt_maintenance_df\"\n###### Objective: (incoming csv files) --> \"apt_maintenance_streamingDF\" --> \"apt_maintenance_df\""],"metadata":{}},{"cell_type":"code","source":["# Get apt_maintenance Steaming DataFrame from csv files\n\n# streaming starts here by reading the input files \napt_maintenance_Path = \"/FileStore/apartment/apartment_maintenance/inprogress/\"\napt_maintenance_streamingDF = (\n  spark\n    .readStream\n    .schema(apt_maintenance_schema)\n    .option(\"maxFilesPerTrigger\", \"1\")\n    .option(\"header\", \"true\")\n    .option(\"multiLine\", \"true\")\n    .csv(apt_maintenance_Path)\n)\n# Clear invalid rows\napt_maintenance_df = apt_maintenance_streamingDF.select(\"*\").where(\"Maintenance_id IS NOT NULL\")\n# Instantiation of DataTransformer class:\ntransformer = op.DataFrameTransformer(apt_maintenance_df)\n# Replace NA with 0's\ntransformer.replace_na(0.0, columns=\"*\")\n# Clear accents: clear_accents only from name column and not everywhere \ntransformer.clear_accents(columns='*')\n# Remove special characters:  From all Columns \n# transformer.remove_special_chars(columns=['apt_maintenance_name', 'Address_line_1', 'City', 'Post_code', 'Region'])"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["##### This function parses the corresponding columns into a single column"],"metadata":{}},{"cell_type":"code","source":["def my_fun(Apartment_number, Mdate, Issue_reported, Contractor_id, Resolution, Status, Charges_incurred, event_time):\n  return zip(Apartment_number, Mdate, Issue_reported, Contractor_id, Resolution, Status, Charges_incurred, event_time)\n\nudf_Fun = udf(my_fun, ArrayType(apt_maintenance_udf_schema))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["intermediate_df = ( apt_maintenance_df.withWatermark(\"event_time\", \"10 seconds\")\n            .groupBy(\"Maintenance_id\")\n            .agg(F.collect_list(\"Apartment_number\").alias(\"Apartment_number\"),\n                 F.collect_list(\"Mdate\").alias(\"Mdate\"),\n                 F.collect_list(\"Issue_reported\").alias(\"Issue_reported\"),\n                 F.collect_list(\"Contractor_id\").alias(\"Contractor_id\"), \n                 F.collect_list(\"Resolution\").alias(\"Resolution\"), \n                 F.collect_list(\"Status\").alias(\"Status\"), \n                 F.collect_list(\"Charges_incurred\").alias(\"Charges_incurred\"), \n                 F.collect_list(\"event_time\").alias(\"event_time\"), \n                 F.max(\"event_time\").alias(\"latest_event_time\"))\n            .select(\"Maintenance_id\", \n                    F.explode(udf_Fun(F.column(\"Apartment_number\"), \n                                      F.column(\"Mdate\"), \n                                      F.column(\"Issue_reported\"), \n                                      F.column(\"Contractor_id\"), \n                                      F.column(\"Resolution\"), \n                                      F.column(\"Status\"), \n                                      F.column(\"Charges_incurred\"), \n                                      F.column(\"event_time\")))\n                    .alias(\"data\"), \"latest_event_time\"))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["##### Filter the data where event_time is latest"],"metadata":{}},{"cell_type":"code","source":["results_df = (intermediate_df\n              .select(\"Maintenance_id\", \n                      \"data.Apartment_number\", \n                      \"data.Mdate\", \n                      \"data.Issue_reported\", \n                      \"data.Contractor_id\", \n                      \"data.Resolution\", \n                      \"data.Status\",\n                      \"data.Charges_incurred\", \n                      \"data.event_time\", \n                      \"latest_event_time\")\n              .where(\"data.event_time=latest_event_time\")).orderBy(\"Maintenance_id\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["##### Display final result\n###### This result shows the latest state of all the unique apt_maintenance_id"],"metadata":{}},{"cell_type":"code","source":["display(results_df)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["##### Below cells are optional if external functionality or storage is needed"],"metadata":{}},{"cell_type":"markdown","source":["###### Write the stream to a Databricks Delta table for storage"],"metadata":{}},{"cell_type":"code","source":["streaming_query = (results_df.writeStream\n .format(\"delta\")\n .outputMode(\"complete\")\n .option(\"mergeSchema\", \"true\")\n .option(\"checkpointLocation\", \"/delta/apartment/apartment_maintenance/_checkpoints/streaming-agg\")\n .start(\"/delta/apartment/apartment_maintenance_data\"))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["#### Read the Delta Table as a Static or Streaming DataFrame\n#### This dataframe will always be Up-To-Date"],"metadata":{}},{"cell_type":"code","source":["apt_maintenance_data = spark.read.format(\"delta\").load(\"/delta/apartment/apartment_maintenance_data\").orderBy(\"Maintenance_id\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(apt_maintenance_data)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Do Some Live Streaming Graphs"],"metadata":{}},{"cell_type":"code","source":["apt_maintenance_data_stream = spark.readStream.format(\"delta\").load(\"/delta/apartment/apartment_maintenance_data\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(apt_maintenance_data_stream.groupBy(\"Status\").count())"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"Apartment_Maintenance_E2E","notebookId":2730151624851845},"nbformat":4,"nbformat_minor":0}
